{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtzCd40vL9G2UqZdqAI1mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e979adf6c05f43ffa3ef9a088b1977d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e04f3bdfcc04521a6845611f1db1250",
              "IPY_MODEL_41b9c0c02209475f9d67bb66a71afba9",
              "IPY_MODEL_62fddcadeb594d2f9a48ba9fc3665fcc"
            ],
            "layout": "IPY_MODEL_39c138058ed34fe083c5673401a84e9d"
          }
        },
        "5e04f3bdfcc04521a6845611f1db1250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0789fe508ca84e89a8ab54b1aa5562ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f6d974b6bada4f7cae15ddb406bad889",
            "value": "Processing papers: 100%"
          }
        },
        "41b9c0c02209475f9d67bb66a71afba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfaa81efd6148be901432065c86183f",
            "max": 6516,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee93bfd4ced249a0bf95f7b2874bc3ad",
            "value": 6516
          }
        },
        "62fddcadeb594d2f9a48ba9fc3665fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6bf8dd13d3b417f96e9285a8a029e28",
            "placeholder": "​",
            "style": "IPY_MODEL_2f2a97f1160d46bfa578eb60e019a910",
            "value": " 6516/6516 [00:00&lt;00:00, 10178.73it/s]"
          }
        },
        "39c138058ed34fe083c5673401a84e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0789fe508ca84e89a8ab54b1aa5562ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d974b6bada4f7cae15ddb406bad889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dfaa81efd6148be901432065c86183f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee93bfd4ced249a0bf95f7b2874bc3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6bf8dd13d3b417f96e9285a8a029e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f2a97f1160d46bfa578eb60e019a910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benshindel/journal-ranking/blob/main/journalrank3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNeH7svkaBGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e979adf6c05f43ffa3ef9a088b1977d9",
            "5e04f3bdfcc04521a6845611f1db1250",
            "41b9c0c02209475f9d67bb66a71afba9",
            "62fddcadeb594d2f9a48ba9fc3665fcc",
            "39c138058ed34fe083c5673401a84e9d",
            "0789fe508ca84e89a8ab54b1aa5562ae",
            "f6d974b6bada4f7cae15ddb406bad889",
            "9dfaa81efd6148be901432065c86183f",
            "ee93bfd4ced249a0bf95f7b2874bc3ad",
            "a6bf8dd13d3b417f96e9285a8a029e28",
            "2f2a97f1160d46bfa578eb60e019a910"
          ]
        },
        "id": "-x-le2DjYAVF",
        "outputId": "7a2f0cc6-37b8-4071-e46c-28249e2c8841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for journal: ACS applied materials & interfaces\n",
            "Found 1 potential matches:\n",
            "1. ACS Applied Materials & Interfaces - ID: https://openalex.org/S164001016 - Publisher: https://openalex.org/P4310320006\n",
            "Selected exact match: ACS Applied Materials & Interfaces (ID: https://openalex.org/S164001016)\n",
            "Fetching papers from 'ACS applied materials & interfaces' (ID: https://openalex.org/S164001016) published in 2023...\n",
            "Using filter: publication_year:2023,primary_location.source.id:https://openalex.org/S164001016\n",
            "Found 5585 papers for 'ACS applied materials & interfaces' in 2023. Fetching 28 pages...\n",
            "Fetched page 1/28 from 'ACS applied materials & interfaces' in 2023 (200 papers so far)\n",
            "Fetched page 2/28 from 'ACS applied materials & interfaces' in 2023 (400 papers so far)\n",
            "Fetched page 3/28 from 'ACS applied materials & interfaces' in 2023 (600 papers so far)\n",
            "Fetched page 4/28 from 'ACS applied materials & interfaces' in 2023 (800 papers so far)\n",
            "Fetched page 5/28 from 'ACS applied materials & interfaces' in 2023 (1000 papers so far)\n",
            "Fetched page 6/28 from 'ACS applied materials & interfaces' in 2023 (1200 papers so far)\n",
            "Fetched page 7/28 from 'ACS applied materials & interfaces' in 2023 (1400 papers so far)\n",
            "Fetched page 8/28 from 'ACS applied materials & interfaces' in 2023 (1600 papers so far)\n",
            "Fetched page 9/28 from 'ACS applied materials & interfaces' in 2023 (1800 papers so far)\n",
            "Fetched page 10/28 from 'ACS applied materials & interfaces' in 2023 (2000 papers so far)\n",
            "Fetched page 11/28 from 'ACS applied materials & interfaces' in 2023 (2200 papers so far)\n",
            "Fetched page 12/28 from 'ACS applied materials & interfaces' in 2023 (2400 papers so far)\n",
            "Fetched page 13/28 from 'ACS applied materials & interfaces' in 2023 (2600 papers so far)\n",
            "Fetched page 14/28 from 'ACS applied materials & interfaces' in 2023 (2800 papers so far)\n",
            "Fetched page 15/28 from 'ACS applied materials & interfaces' in 2023 (3000 papers so far)\n",
            "Fetched page 16/28 from 'ACS applied materials & interfaces' in 2023 (3200 papers so far)\n",
            "Fetched page 17/28 from 'ACS applied materials & interfaces' in 2023 (3400 papers so far)\n",
            "Fetched page 18/28 from 'ACS applied materials & interfaces' in 2023 (3600 papers so far)\n",
            "Fetched page 19/28 from 'ACS applied materials & interfaces' in 2023 (3800 papers so far)\n",
            "Fetched page 20/28 from 'ACS applied materials & interfaces' in 2023 (4000 papers so far)\n",
            "Fetched page 21/28 from 'ACS applied materials & interfaces' in 2023 (4200 papers so far)\n",
            "Fetched page 22/28 from 'ACS applied materials & interfaces' in 2023 (4400 papers so far)\n",
            "Fetched page 23/28 from 'ACS applied materials & interfaces' in 2023 (4600 papers so far)\n",
            "Fetched page 24/28 from 'ACS applied materials & interfaces' in 2023 (4800 papers so far)\n",
            "Fetched page 25/28 from 'ACS applied materials & interfaces' in 2023 (5000 papers so far)\n",
            "Fetched page 26/28 from 'ACS applied materials & interfaces' in 2023 (5200 papers so far)\n",
            "Fetched page 27/28 from 'ACS applied materials & interfaces' in 2023 (5400 papers so far)\n",
            "Fetched page 28/28 from 'ACS applied materials & interfaces' in 2023 (5585 papers so far)\n",
            "Successfully fetched 5585 papers from 'ACS applied materials & interfaces' in 2023.\n",
            "Searching for journal: Nanotechnology\n",
            "Found 5 potential matches:\n",
            "1. Nature Nanotechnology - ID: https://openalex.org/S7822423 - Publisher: https://openalex.org/P4310319908\n",
            "2. Nanotechnology - ID: https://openalex.org/S206811868 - Publisher: https://openalex.org/P4310320083\n",
            "3. Journal of Nanoscience and Nanotechnology - ID: https://openalex.org/S133947007 - Publisher: https://openalex.org/P4310321646\n",
            "4. Nanomedicine Nanotechnology Biology and Medicine - ID: https://openalex.org/S80729884 - Publisher: https://openalex.org/P4310320990\n",
            "5. IEEE Transactions on Nanotechnology - ID: https://openalex.org/S142331907 - Publisher: https://openalex.org/P4310319808\n",
            "Selected exact match: Nanotechnology (ID: https://openalex.org/S206811868)\n",
            "Fetching papers from 'Nanotechnology' (ID: https://openalex.org/S206811868) published in 2023...\n",
            "Using filter: publication_year:2023,primary_location.source.id:https://openalex.org/S206811868\n",
            "Found 830 papers for 'Nanotechnology' in 2023. Fetching 5 pages...\n",
            "Fetched page 1/5 from 'Nanotechnology' in 2023 (200 papers so far)\n",
            "Fetched page 2/5 from 'Nanotechnology' in 2023 (400 papers so far)\n",
            "Fetched page 3/5 from 'Nanotechnology' in 2023 (600 papers so far)\n",
            "Fetched page 4/5 from 'Nanotechnology' in 2023 (800 papers so far)\n",
            "Fetched page 5/5 from 'Nanotechnology' in 2023 (830 papers so far)\n",
            "Successfully fetched 830 papers from 'Nanotechnology' in 2023.\n",
            "Searching for journal: Beilstein Journal of Nanotechnology\n",
            "Found 1 potential matches:\n",
            "1. Beilstein Journal of Nanotechnology - ID: https://openalex.org/S2504426957 - Publisher: https://openalex.org/P4310320289\n",
            "Selected exact match: Beilstein Journal of Nanotechnology (ID: https://openalex.org/S2504426957)\n",
            "Fetching papers from 'Beilstein Journal of Nanotechnology' (ID: https://openalex.org/S2504426957) published in 2023...\n",
            "Using filter: publication_year:2023,primary_location.source.id:https://openalex.org/S2504426957\n",
            "Found 101 papers for 'Beilstein Journal of Nanotechnology' in 2023. Fetching 1 pages...\n",
            "Fetched page 1/1 from 'Beilstein Journal of Nanotechnology' in 2023 (101 papers so far)\n",
            "Successfully fetched 101 papers from 'Beilstein Journal of Nanotechnology' in 2023.\n",
            "Total papers fetched from all journals and years: 6516\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing papers:   0%|          | 0/6516 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e979adf6c05f43ffa3ef9a088b1977d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved to Colab filesystem: ACS_journals_2023-2023_papers.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fd3e81fc-47d4-4727-ba27-0408c1462ddc\", \"ACS_journals_2023-2023_papers.xlsx\", 6037002)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded file: ACS_journals_2023-2023_papers.xlsx\n",
            "\n",
            "Summary (Initial Spreadsheet - Papers from Multiple ACS Journals for Year(s) 2023-2023):\n",
            "- Journals: ACS applied materials & interfaces, Nanotechnology, Beilstein Journal of Nanotechnology\n",
            "- Year(s): 2023-2023\n",
            "- Papers retrieved: 6516\n",
            "- Initial Spreadsheet saved to: ACS_journals_2023-2023_papers.xlsx AND Downloaded\n",
            "- Data columns: title, doi, publication_date, publication_year, open_access, cited_by_count, url, volume, issue, type, is_retracted, is_paratext, journal, journal_issn, publisher, language, countries, alternate_urls, topics, open_access_status, open_access_url, corresponding_author, corresponding_institution, citations_per_year, referenced_works_count, grants, authors, institutions, author_count, concepts, abstract, mesh_terms\n",
            "\n",
            "Article Type Distribution (Initial Data):\n",
            "- article: 6269 (96.2%)\n",
            "- review: 139 (2.1%)\n",
            "- erratum: 58 (0.9%)\n",
            "- paratext: 26 (0.4%)\n",
            "- editorial: 19 (0.3%)\n",
            "- retraction: 4 (0.1%)\n",
            "- letter: 1 (0.0%)\n",
            "\n",
            "Initial spreadsheet generation and download completed. File saved at: ACS_journals_2023-2023_papers.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "from google.colab import files # Import files for direct download\n",
        "\n",
        "# --- Include all your original functions here (find_source_id, extract_paper_info, save_to_excel) ---\n",
        "# ---  `find_source_id`, `extract_paper_info`, and `save_to_excel` remain UNCHANGED  ---\n",
        "# --- `fetch_papers` and `fetch_journal_papers_spreadsheet` are MODIFIED to accept year(s) as input ---\n",
        "\n",
        "\n",
        "# Function to find the source ID for a journal (NO CHANGE)\n",
        "def find_source_id(journal_name, email):\n",
        "    headers = {\n",
        "        \"User-Agent\": f\"Python Script ({email})\"\n",
        "    }\n",
        "\n",
        "    # Search for the journal by name\n",
        "    search_url = \"https://api.openalex.org/sources\"\n",
        "    params = {\n",
        "        \"search\": journal_name,\n",
        "        \"per_page\": 5\n",
        "    }\n",
        "\n",
        "    print(f\"Searching for journal: {journal_name}\")\n",
        "    response = requests.get(search_url, params=params, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error searching for journal: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "    data = response.json()\n",
        "    results = data.get(\"results\", [])\n",
        "\n",
        "    if not results:\n",
        "        print(f\"No journals found matching '{journal_name}'\")\n",
        "        return None\n",
        "\n",
        "    # Display the top results\n",
        "    print(f\"Found {len(results)} potential matches:\")\n",
        "    for i, source in enumerate(results):\n",
        "        print(f\"{i+1}. {source.get('display_name')} - ID: {source.get('id')} - Publisher: {source.get('host_organization')}\")\n",
        "\n",
        "    # For automation, we'll take the first result if it's an exact match\n",
        "    for source in results:\n",
        "        if source.get(\"display_name\").lower() == journal_name.lower():\n",
        "            source_id = source.get(\"id\")\n",
        "            print(f\"Selected exact match: {source.get('display_name')} (ID: {source_id})\")\n",
        "            return source_id\n",
        "\n",
        "    # If no exact match, use the first result\n",
        "    source_id = results[0].get(\"id\")\n",
        "    print(f\"Selected best match: {results[0].get('display_name')} (ID: {source_id})\")\n",
        "    return source_id\n",
        "\n",
        "# Function to extract relevant fields from paper metadata (NO CHANGE)\n",
        "def extract_paper_info(papers):\n",
        "    extracted_data = []\n",
        "\n",
        "    for paper in tqdm(papers, desc=\"Processing papers\"):\n",
        "        # Extract basic information\n",
        "        paper_info = {\n",
        "            \"title\": paper.get(\"title\", \"\"),\n",
        "            \"doi\": paper.get(\"doi\", \"\"),\n",
        "            \"publication_date\": paper.get(\"publication_date\", \"\"),\n",
        "            \"publication_year\": paper.get(\"publication_year\", \"\"),\n",
        "            \"open_access\": paper.get(\"open_access\", {}).get(\"is_oa\", False),\n",
        "            \"cited_by_count\": paper.get(\"cited_by_count\", 0),\n",
        "            \"url\": paper.get(\"primary_location\", {}).get(\"landing_page_url\", \"\"),\n",
        "            \"volume\": paper.get(\"biblio\", {}).get(\"volume\", \"\"),\n",
        "            \"issue\": paper.get(\"biblio\", {}).get(\"issue\", \"\"),\n",
        "\n",
        "            # Added new metadata fields\n",
        "            \"type\": paper.get(\"type\", \"\"),  # Article type (e.g., article, review, etc.)\n",
        "            \"is_retracted\": paper.get(\"is_retracted\", False),\n",
        "            \"is_paratext\": paper.get(\"is_paratext\", False),\n",
        "            \"journal\": paper.get(\"primary_location\", {}).get(\"source\", {}).get(\"display_name\", \"\"),\n",
        "            \"journal_issn\": paper.get(\"primary_location\", {}).get(\"source\", {}).get(\"issn_l\", \"\"),\n",
        "            \"publisher\": paper.get(\"primary_location\", {}).get(\"host_organization_name\", \"\"),\n",
        "            \"language\": paper.get(\"language\", \"\"),\n",
        "            \"countries\": \", \".join([country.get(\"display_name\", \"\") for country in paper.get(\"countries_distinct_names\", [])]),\n",
        "            \"alternate_urls\": \", \".join([loc.get(\"landing_page_url\", \"\") for loc in paper.get(\"locations\", [])[:3] if loc.get(\"landing_page_url\")]),\n",
        "            \"topics\": \", \".join([topic.get(\"display_name\", \"\") for topic in paper.get(\"topics\", [])[:5]]),\n",
        "            \"open_access_status\": paper.get(\"open_access\", {}).get(\"oa_status\", \"\"),\n",
        "            \"open_access_url\": paper.get(\"open_access\", {}).get(\"oa_url\", \"\"),\n",
        "            \"corresponding_author\": \"\",\n",
        "            \"corresponding_institution\": \"\",\n",
        "            \"citations_per_year\": \", \".join([f\"{year}: {count}\" for year, count in paper.get(\"counts_by_year\", [{}])[:5]]),\n",
        "            \"referenced_works_count\": paper.get(\"referenced_works_count\", 0),\n",
        "            \"grants\": \", \".join([grant.get(\"funder_display_name\", \"\") for grant in paper.get(\"grants\", [])[:3]]),\n",
        "        }\n",
        "\n",
        "        # Extract authors (up to 5)\n",
        "        authors = paper.get(\"authorships\", [])\n",
        "        author_names = []\n",
        "        author_institutions = []\n",
        "\n",
        "        # Look for corresponding author info\n",
        "        for i, author in enumerate(authors):\n",
        "            name = author.get(\"author\", {}).get(\"display_name\", \"\")\n",
        "            if name:\n",
        "                author_names.append(name)\n",
        "\n",
        "            institutions = []\n",
        "            for inst in author.get(\"institutions\", []):\n",
        "                inst_name = inst.get(\"display_name\", \"\")\n",
        "                if inst_name:\n",
        "                    institutions.append(inst_name)\n",
        "\n",
        "            if institutions:\n",
        "                author_institutions.append(\"; \".join(institutions))\n",
        "\n",
        "            # Check if this is a corresponding author\n",
        "            if author.get(\"is_corresponding\", False):\n",
        "                paper_info[\"corresponding_author\"] = name\n",
        "                paper_info[\"corresponding_institution\"] = \"; \".join(institutions) if institutions else \"\"\n",
        "\n",
        "        paper_info[\"authors\"] = \", \".join(author_names[:5])\n",
        "        paper_info[\"institutions\"] = \" | \".join(author_institutions[:5])\n",
        "        paper_info[\"author_count\"] = len(authors)\n",
        "\n",
        "        # Extract concepts/keywords (up to 5 main ones)\n",
        "        concepts = paper.get(\"concepts\", [])\n",
        "        concept_names = []\n",
        "\n",
        "        for concept in sorted(concepts, key=lambda x: x.get(\"score\", 0), reverse=True)[:5]:\n",
        "            score = concept.get(\"score\", 0)\n",
        "            name = concept.get(\"display_name\", \"\")\n",
        "            if name:\n",
        "                concept_names.append(f\"{name} ({score:.2f})\")\n",
        "\n",
        "        paper_info[\"concepts\"] = \", \".join(concept_names)\n",
        "\n",
        "        # Extract abstract\n",
        "        paper_info[\"abstract\"] = paper.get(\"abstract_inverted_index\", {})\n",
        "        if paper_info[\"abstract\"]:\n",
        "            try:\n",
        "                # Convert inverted index to text (OpenAlex specific format)\n",
        "                words = list(paper_info[\"abstract\"].keys())\n",
        "                positions = {}\n",
        "\n",
        "                for word, pos_list in paper_info[\"abstract\"].items():\n",
        "                    for pos in pos_list:\n",
        "                        positions[pos] = word\n",
        "\n",
        "                abstract_text = \" \".join([positions.get(i, \"\") for i in range(max(positions.keys()) + 1)])\n",
        "                paper_info[\"abstract\"] = abstract_text\n",
        "            except:\n",
        "                paper_info[\"abstract\"] = \"Error processing abstract\"\n",
        "        else:\n",
        "            paper_info[\"abstract\"] = \"\"\n",
        "\n",
        "        # Get mesh terms if available (medical papers)\n",
        "        mesh_terms = []\n",
        "        for mesh in paper.get(\"mesh\", [])[:5]:\n",
        "            qualifier = mesh.get(\"qualifier_name\", \"\")\n",
        "            descriptor = mesh.get(\"descriptor_name\", \"\")\n",
        "            if descriptor:\n",
        "                term = descriptor\n",
        "                if qualifier:\n",
        "                    term += f\" ({qualifier})\"\n",
        "                mesh_terms.append(term)\n",
        "\n",
        "        paper_info[\"mesh_terms\"] = \", \".join(mesh_terms)\n",
        "\n",
        "        extracted_data.append(paper_info)\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# Function to save data to Excel in Colab's local filesystem AND provide download link (NO CHANGE)\n",
        "def save_to_excel(data, filepath, journal=\"ACS_Journals_YEAR_PAPERS\", year=\"YEAR\"): # Generic journal name, updated default filename prefix - year can now be string\n",
        "    \"\"\"\n",
        "    Save paper data to Excel in Colab's local filesystem AND provide download link.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : list\n",
        "        List of paper information dictionaries\n",
        "    filepath : str\n",
        "        Filepath to save the Excel file to within Colab.\n",
        "    journal : str\n",
        "        Journal name prefix for filename (now generic \"ACS_Journals_YEAR_PAPERS\")\n",
        "    year : str or int\n",
        "        Publication year or year range for filename (can be string now)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str: The filepath where the Excel file was saved.\n",
        "    \"\"\"\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save to Excel in Colab filesystem\n",
        "    df.to_excel(filepath, index=False, engine='openpyxl')\n",
        "    print(f\"File saved to Colab filesystem: {filepath}\")\n",
        "\n",
        "    # --- ADDED: Download the file immediately ---\n",
        "    try:\n",
        "        files.download(filepath) # Trigger download\n",
        "        print(f\"Downloaded file: {filepath}\")\n",
        "    except ImportError:\n",
        "        print(\"`files.download` not available (not in Colab). File is saved in Colab filesystem but not automatically downloaded.\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "\n",
        "# --- MODIFIED fetch_papers to fetch from a LIST of journals for SPECIFIED year(s) ---\n",
        "def fetch_papers(journal_names, year, email=\"benshindel@gmail.com\"): # year can now be single year or a LIST of years\n",
        "    headers = {\n",
        "        \"User-Agent\": f\"Python Script ({email})\"\n",
        "    }\n",
        "\n",
        "    all_journal_papers = [] # List to hold papers from ALL journals\n",
        "\n",
        "    # Check if year is a list or a single year\n",
        "    if isinstance(year, list):\n",
        "        years_to_fetch = year\n",
        "    else:\n",
        "        years_to_fetch = [year] # If single year, make it a list for iteration\n",
        "\n",
        "    for journal_name in journal_names: # Iterate through each journal name\n",
        "        source_id = find_source_id(journal_name, email) # Find source ID for EACH journal\n",
        "\n",
        "        if not source_id:\n",
        "            print(f\"Could not find source ID for journal '{journal_name}'. Skipping.\")\n",
        "            continue # Skip to the next journal if source ID not found\n",
        "\n",
        "        for current_year in years_to_fetch: # Iterate through each year to fetch\n",
        "            # Base URL for OpenAlex API\n",
        "            base_url = \"https://api.openalex.org/works\"\n",
        "\n",
        "            # Filter for publication year AND source ID\n",
        "            filters = [\n",
        "                f\"publication_year:{current_year}\",\n",
        "                f\"primary_location.source.id:{source_id}\"\n",
        "            ]\n",
        "            filter_param = \",\".join(filters)\n",
        "\n",
        "            # Parameters for API request\n",
        "            params = {\n",
        "                \"filter\": filter_param,\n",
        "                \"per_page\": 200 # Max per page\n",
        "            }\n",
        "\n",
        "            journal_results = [] # List to store results for CURRENT journal AND year\n",
        "            page = 1\n",
        "            total_pages = None\n",
        "\n",
        "            print(f\"Fetching papers from '{journal_name}' (ID: {source_id}) published in {current_year}...\") # Updated print for year\n",
        "            print(f\"Using filter: {filter_param}\")\n",
        "\n",
        "            while total_pages is None or page <= total_pages:\n",
        "                params[\"page\"] = page\n",
        "                response = requests.get(base_url, params=params, headers=headers)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"Error: API returned status code {response.status_code}\")\n",
        "                    print(response.text)\n",
        "                    break\n",
        "\n",
        "                data = response.json()\n",
        "\n",
        "                if total_pages is None:\n",
        "                    total_count = data.get(\"meta\", {}).get(\"count\", 0)\n",
        "                    total_pages = (total_count + params[\"per_page\"] - 1) // params[\"per_page\"]\n",
        "                    print(f\"Found {total_count} papers for '{journal_name}' in {current_year}. Fetching {total_pages} pages...\") # Updated print for year\n",
        "\n",
        "                    if total_count == 0:\n",
        "                        print(f\"No papers found for '{journal_name}' in {current_year} matching criteria.\") # Updated print for year\n",
        "                        break # No papers for this journal in this year, move to the next year\n",
        "\n",
        "                journal_results.extend(data.get(\"results\", []))\n",
        "                print(f\"Fetched page {page}/{total_pages} from '{journal_name}' in {current_year} ({len(journal_results)} papers so far)\") # Updated print for year\n",
        "                page += 1\n",
        "                time.sleep(0.2) # Rate limiting\n",
        "\n",
        "            print(f\"Successfully fetched {len(journal_results)} papers from '{journal_name}' in {current_year}.\") # Updated print for year\n",
        "            all_journal_papers.extend(journal_results) # Add results for this journal and year to the overall list\n",
        "\n",
        "    print(f\"Total papers fetched from all journals and years: {len(all_journal_papers)}\") # Updated total count message\n",
        "    return all_journal_papers\n",
        "\n",
        "\n",
        "# --- MODIFIED Main execution function for STEP 1: Generate and Save Spreadsheet for MULTIPLE journals and SPECIFIED year(s) ---\n",
        "def fetch_journal_papers_spreadsheet(journal_names, year, email=\"benshindel@gmail.com\", output_filepath=\"ACS_journals_YEAR_papers.xlsx\"): # year can now be single year or list, generic filename with YEAR placeholder\n",
        "    \"\"\"\n",
        "    Main function to fetch and process papers from a list of journals for given year(s), and save to Excel.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    journal_names : list\n",
        "        List of journal names to fetch papers from.\n",
        "    year : int or list\n",
        "        Publication year or list of years to fetch papers from.\n",
        "    email : str\n",
        "        Email for user agent (default: \"benshindel@gmail.com\")\n",
        "    output_filepath : str\n",
        "        Filepath to save the initial spreadsheet (default: \"ACS_journals_YEAR_papers.xlsx\") - YEAR will be replaced\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str: Filepath of the saved spreadsheet.\n",
        "    \"\"\"\n",
        "    # Step 1: Fetch papers from ALL journals in the list for specified year(s)\n",
        "    all_papers = fetch_papers(journal_names, year, email) # Fetch for list of journals and year(s)\n",
        "\n",
        "    if not all_papers:\n",
        "        print(\"No data to process.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Extract relevant information\n",
        "    paper_data = extract_paper_info(all_papers)\n",
        "\n",
        "    # Step 3: Save to Excel in Colab filesystem AND download\n",
        "\n",
        "    # Determine year string for filename - handle single year or year range in filename\n",
        "    if isinstance(year, list):\n",
        "        year_str = f\"{min(year)}-{max(year)}\" # e.g., 2019-2022 if years = [2019, 2020, 2021, 2022]\n",
        "    else:\n",
        "        year_str = str(year) # e.g., 2024 if year = 2024\n",
        "\n",
        "    output_filepath_final = output_filepath.replace(\"YEAR\", year_str) # Replace \"YEAR\" placeholder in filename\n",
        "    excel_filepath = save_to_excel(paper_data, output_filepath_final, journal=f\"ACS_Journals_{year_str}_Papers\", year=year_str) # Generic journal name for filename, use year_str\n",
        "\n",
        "\n",
        "    # Create DataFrame (optional, for summary)\n",
        "    df = pd.DataFrame(paper_data)\n",
        "\n",
        "    # Display summary\n",
        "    print(f\"\\nSummary (Initial Spreadsheet - Papers from Multiple ACS Journals for Year(s) {year_str}):\") # Updated summary message for year(s)\n",
        "    print(f\"- Journals: {', '.join(journal_names)}\") # List journal names in summary\n",
        "    print(f\"- Year(s): {year_str}\") # Show year(s) in summary\n",
        "    print(f\"- Papers retrieved: {len(all_papers)}\")\n",
        "    print(f\"- Initial Spreadsheet saved to: {excel_filepath} AND Downloaded\")\n",
        "    print(f\"- Data columns: {', '.join(df.columns)}\")\n",
        "\n",
        "     # Add type distribution summary\n",
        "    if 'type' in df.columns:\n",
        "        print(\"\\nArticle Type Distribution (Initial Data):\")\n",
        "        type_counts = df['type'].value_counts()\n",
        "        for type_name, count in type_counts.items():\n",
        "            print(f\"- {type_name}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "    return excel_filepath # Return filepath\n",
        "\n",
        "\n",
        "# --- EXECUTION CELL for STEP 1: Generate, Save, and Download Spreadsheet for MULTIPLE journals and SPECIFIED year(s) ---\n",
        "journal_names_to_fetch = [\n",
        "    \"ACS applied materials & interfaces\",\n",
        "    \"Nanotechnology\",\n",
        "    \"Beilstein Journal of Nanotechnology\"\n",
        "] # List of journal names\n",
        "\n",
        "years_to_fetch = [2023] # Specify year(s) as a list OR a single year like: years_to_fetch = 2024\n",
        "# years_to_fetch = 2024 # Example for single year\n",
        "\n",
        "initial_spreadsheet_filepath = fetch_journal_papers_spreadsheet(journal_names=journal_names_to_fetch, year=years_to_fetch, email=\"benshindel@gmail.com\", output_filepath=\"ACS_journals_YEAR_papers.xlsx\") # Pass year(s)\n",
        "\n",
        "print(f\"\\nInitial spreadsheet generation and download completed. File saved at: {initial_spreadsheet_filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files # Import files for download in the cleaning step\n",
        "import numpy as np # Import numpy for NaN check (though pd.isna works without explicit import now in recent pandas versions, it's good practice to import numpy if dealing with NaNs)\n",
        "\n",
        "\n",
        "# --- SIMPLIFIED CLEAN DATAFRAME FUNCTION (Direct NaN Check - NO CHANGES) ---\n",
        "def clean_dataframe(df, volume_column_name=\"volume\", institution_column_name=\"institutions\"):\n",
        "    \"\"\"\n",
        "    Cleans a Pandas DataFrame by removing rows that have NaN in volume or institution columns.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The Pandas DataFrame to clean.\n",
        "        volume_column_name (str): The name of the volume column.\n",
        "        institution_column_name (str): The name of the institution column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned Pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Cleaning DataFrame...\")\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty, nothing to clean.\")\n",
        "        return df\n",
        "\n",
        "    rows_to_drop_indices = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        is_volume_blank = False\n",
        "        is_institution_blank = False\n",
        "\n",
        "        # Check for NaN in volume column\n",
        "        if volume_column_name in df.columns:\n",
        "            if pd.isna(row[volume_column_name]): # Direct NaN check using pd.isna()\n",
        "                is_volume_blank = True\n",
        "\n",
        "        # Check for NaN in institution column\n",
        "        if institution_column_name in df.columns:\n",
        "            if pd.isna(row[institution_column_name]): # Direct NaN check using pd.isna()\n",
        "                is_institution_blank = True\n",
        "\n",
        "        # Delete row if volume OR institution is NaN\n",
        "        if is_volume_blank or is_institution_blank:\n",
        "            rows_to_drop_indices.append(index)\n",
        "            # print(f\"Row {index} marked for deletion (NaN in volume or institution).\") # Optional: Minimal deletion message\n",
        "\n",
        "    if rows_to_drop_indices:\n",
        "        print(f\"Identified {len(rows_to_drop_indices)} rows to delete from DataFrame (NaN values).\")\n",
        "        cleaned_df = df.drop(index=rows_to_drop_indices)\n",
        "        print(f\"DataFrame cleaned. {len(rows_to_drop_indices)} rows removed.\")\n",
        "        return cleaned_df\n",
        "    else:\n",
        "        print(\"No rows found to delete based on criteria (NaN values).\")\n",
        "        return df\n",
        "\n",
        "\n",
        "# Main function for cleaning the GENERATED spreadsheet file (NO CHANGES)\n",
        "def clean_generated_spreadsheet(input_filepath=\"ACS_journals_2023-2023_papers.xlsx\", output_filepath=\"cleaned_papers.xlsx\"): # Updated default input filename\n",
        "    \"\"\"\n",
        "    Cleans the Excel spreadsheet generated by the first step.\n",
        "\n",
        "    Args:\n",
        "        input_filepath (str): Filepath of the initial spreadsheet.\n",
        "        output_filepath (str): Filepath to save the cleaned spreadsheet.\n",
        "\n",
        "    Returns:\n",
        "        str: Filepath to the cleaned spreadsheet.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Loading spreadsheet for cleaning from: {input_filepath}\")\n",
        "        initial_df = pd.read_excel(input_filepath)\n",
        "\n",
        "        # Clean the DataFrame\n",
        "        cleaned_df = clean_dataframe(initial_df)\n",
        "\n",
        "        # Save the cleaned DataFrame to a NEW Excel file\n",
        "        cleaned_filepath = cleaned_df.to_excel(output_filepath, index=False, engine='openpyxl') # Save and get None return\n",
        "\n",
        "        cleaned_filepath = output_filepath # Correctly set cleaned_filepath to the output path\n",
        "\n",
        "        print(f\"Cleaned spreadsheet saved to: {cleaned_filepath}\")\n",
        "\n",
        "        # Optional: Download the cleaned spreadsheet\n",
        "        try:\n",
        "            files.download(output_filepath)\n",
        "            print(f\"Downloaded cleaned spreadsheet: {output_filepath}\")\n",
        "        except ImportError:\n",
        "            print(\"`files.download` not available (not in Colab). File is saved in Colab filesystem.\")\n",
        "\n",
        "        return cleaned_filepath\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at: {input_filepath}. Make sure you ran the first code block and the file was saved.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during spreadsheet cleaning: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- EXECUTION CELL for STEP 2: Clean the Spreadsheet generated in Step 1 (NO CHANGES) ---\n",
        "cleaned_spreadsheet_filepath = clean_generated_spreadsheet(input_filepath=\"ACS_journals_2023-2023_papers.xlsx\", output_filepath=\"cleaned_papers.xlsx\") # Updated default input filename\n",
        "\n",
        "if cleaned_spreadsheet_filepath:\n",
        "    print(f\"\\nSpreadsheet cleaning completed. Cleaned file saved at: {cleaned_spreadsheet_filepath}\")\n",
        "else:\n",
        "    print(\"\\nSpreadsheet cleaning process failed. See error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "lQZDPipv9hHk",
        "outputId": "339a0463-fab0-44ab-976d-2259f930ad91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading spreadsheet for cleaning from: ACS_journals_2023-2023_papers.xlsx\n",
            "Cleaning DataFrame...\n",
            "Identified 534 rows to delete from DataFrame (NaN values).\n",
            "DataFrame cleaned. 534 rows removed.\n",
            "Cleaned spreadsheet saved to: cleaned_papers.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c6c5facd-79c2-4e2f-b452-eedb3d384e9d\", \"cleaned_papers.xlsx\", 5597413)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded cleaned spreadsheet: cleaned_papers.xlsx\n",
            "\n",
            "Spreadsheet cleaning completed. Cleaned file saved at: cleaned_papers.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files # For downloading the labeled spreadsheet\n",
        "\n",
        "# --- CODE BLOCK 3: Labeling Spreadsheet with \"Top Institution\" Column ---\n",
        "def label_top_institution(input_filepath=\"cleaned_papers.xlsx\", output_filepath=\"labeled_papers.xlsx\", top_institution_strings=[\"Harvard University\", \"Stanford University\", \"Massachusetts Institute of Technology\", \"\tTsinghua University\", \"University of Oxford\", \"Princeton University\", \"University of Cambridge\", \"Johns Hopkins University\", \"National University of Singapore\"]):\n",
        "    \"\"\"\n",
        "    Adds a \"Top_Institution\" column to the cleaned spreadsheet, labeling \"YES\" if\n",
        "    any of the specified top institution strings are found in the 'institutions' column,\n",
        "    and \"NO\" otherwise.\n",
        "\n",
        "    Args:\n",
        "        input_filepath (str): Filepath to the cleaned spreadsheet (default: \"cleaned_papers.xlsx\").\n",
        "        output_filepath (str): Filepath to save the labeled spreadsheet (default: \"labeled_papers.xlsx\").\n",
        "        top_institution_strings (list): List of strings representing top institutions to search for.\n",
        "\n",
        "    Returns:\n",
        "        str: Filepath to the labeled spreadsheet.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Loading cleaned spreadsheet for labeling from: {input_filepath}\")\n",
        "        cleaned_df = pd.read_excel(input_filepath)\n",
        "\n",
        "        if cleaned_df.empty:\n",
        "            print(\"Warning: Cleaned DataFrame is empty. No labeling to perform.\")\n",
        "            return None  # Or return input_filepath if you still want to save it as labeled\n",
        "\n",
        "        # Initialize the new column with \"NO\" as default\n",
        "        cleaned_df['Top_Institution'] = \"NO\"\n",
        "\n",
        "        # Convert 'institutions' column to string type to handle potential mixed types and avoid errors during string operations\n",
        "        cleaned_df['institutions'] = cleaned_df['institutions'].astype(str)\n",
        "\n",
        "        # Iterate through each row and check for top institution strings\n",
        "        for index, row in cleaned_df.iterrows():\n",
        "            institutions_text = row['institutions'].lower() # Convert to lowercase for case-insensitive matching\n",
        "            for top_institution_string in top_institution_strings:\n",
        "                if top_institution_string.lower() in institutions_text: # Case-insensitive check\n",
        "                    cleaned_df.at[index, 'Top_Institution'] = \"YES\" # Use .at for efficient value setting\n",
        "                    break # No need to check other strings once a match is found\n",
        "\n",
        "        # Save the labeled DataFrame to a new Excel file\n",
        "        labeled_filepath = cleaned_df.to_excel(output_filepath, index=False, engine='openpyxl') # Save and get None return\n",
        "        labeled_filepath = output_filepath # Correctly set labeled_filepath\n",
        "\n",
        "        print(f\"Labeled spreadsheet saved to: {labeled_filepath}\")\n",
        "\n",
        "        # Optional: Download the labeled spreadsheet\n",
        "        try:\n",
        "            files.download(output_filepath)\n",
        "            print(f\"Downloaded labeled spreadsheet: {output_filepath}\")\n",
        "        except ImportError:\n",
        "            print(\"`files.download` not available (not in Colab). File is saved in Colab filesystem.\")\n",
        "\n",
        "        return labeled_filepath\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at: {input_filepath}. Make sure you ran Code Block 2 and the file was saved.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during spreadsheet labeling: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- EXECUTION CELL for STEP 3: Label Spreadsheet and Download ---\n",
        "labeled_spreadsheet_filepath = label_top_institution(input_filepath=\"cleaned_papers.xlsx\", output_filepath=\"labeled_papers.xlsx\")\n",
        "\n",
        "if labeled_spreadsheet_filepath:\n",
        "    print(f\"\\nSpreadsheet labeling completed. Labeled file saved at: {labeled_spreadsheet_filepath}\")\n",
        "else:\n",
        "    print(\"\\nSpreadsheet labeling process failed. See error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8SELNBj-A_aG",
        "outputId": "fdbd8cc9-a569-4b75-c83d-bd78b08cd13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cleaned spreadsheet for labeling from: cleaned_papers.xlsx\n",
            "Labeled spreadsheet saved to: labeled_papers.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_06624377-997f-4e53-b945-8c18a24e67f2\", \"labeled_papers.xlsx\", 5635867)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded labeled spreadsheet: labeled_papers.xlsx\n",
            "\n",
            "Spreadsheet labeling completed. Labeled file saved at: labeled_papers.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files # For downloading the journal ratios spreadsheet\n",
        "\n",
        "# --- CODE BLOCK 6: Calculate and Output Journal YEARLY Top Institution Ratios WITH COUNTS ---\n",
        "def calculate_journal_yearly_top_institution_ratios(input_filepath=\"labeled_papers.xlsx\", output_filepath=\"journal_yearly_ratios_with_counts.xlsx\"):\n",
        "    \"\"\"\n",
        "    Calculates the ratio of papers with \"Top_Institution\" = \"YES\" for each journal\n",
        "    AND each year present in the labeled spreadsheet. Outputs results to a new spreadsheet\n",
        "    with years as columns, including total paper counts and YES counts.\n",
        "    Also adds total columns at the end for each journal across all years.\n",
        "\n",
        "    Args:\n",
        "        input_filepath (str): Filepath to the labeled spreadsheet (default: \"labeled_papers.xlsx\").\n",
        "        output_filepath (str): Filepath to save the journal yearly ratios spreadsheet (default: \"journal_yearly_ratios_with_counts.xlsx\").\n",
        "\n",
        "    Returns:\n",
        "        str: Filepath to the journal yearly ratios spreadsheet.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Loading labeled spreadsheet for yearly ratio and count calculation from: {input_filepath}\")\n",
        "        labeled_df = pd.read_excel(input_filepath)\n",
        "\n",
        "        if labeled_df.empty:\n",
        "            print(\"Warning: Labeled DataFrame is empty. Cannot calculate ratios or counts.\")\n",
        "            return None\n",
        "\n",
        "        journal_names = labeled_df['journal'].unique() # Get unique journal names\n",
        "        years = sorted(labeled_df['publication_year'].unique()) # Get unique years and sort them\n",
        "\n",
        "        yearly_ratios_data = [] # List to store data for each journal (will be rows in output)\n",
        "\n",
        "        for journal_name in journal_names:\n",
        "            journal_ratios = {'Journal': journal_name} # Dictionary to store ratios and counts for each year for this journal\n",
        "            total_top_institution_papers_all_years = 0 # Initialize total YES counts for all years\n",
        "            total_journal_papers_all_years = 0 # Initialize total paper counts for all years\n",
        "\n",
        "            for year in years:\n",
        "                journal_year_df = labeled_df[(labeled_df['journal'] == journal_name) & (labeled_df['publication_year'] == year)]\n",
        "                total_journal_year_papers = len(journal_year_df)\n",
        "                top_institution_papers = len(journal_year_df[journal_year_df['Top_Institution'] == \"YES\"])\n",
        "\n",
        "                ratio = 0 # Default ratio\n",
        "                if total_journal_year_papers > 0:\n",
        "                    ratio = top_institution_papers / total_journal_year_papers\n",
        "\n",
        "                journal_ratios[f'{year}_Ratio'] = ratio # Store ratio, use year_Ratio as column name\n",
        "                journal_ratios[f'{year}_TotalPapers'] = total_journal_year_papers # Store total papers count\n",
        "                journal_ratios[f'{year}_YesCount'] = top_institution_papers # Store YES count\n",
        "\n",
        "                total_top_institution_papers_all_years += top_institution_papers # Accumulate YES counts\n",
        "                total_journal_papers_all_years += total_journal_year_papers # Accumulate total paper counts\n",
        "\n",
        "            # Calculate total ratio across all years for this journal\n",
        "            total_ratio_all_years = 0\n",
        "            if total_journal_papers_all_years > 0:\n",
        "                total_ratio_all_years = total_top_institution_papers_all_years / total_journal_papers_all_years\n",
        "\n",
        "            # Add total columns to the journal_ratios dictionary\n",
        "            journal_ratios['Total_Ratio'] = total_ratio_all_years\n",
        "            journal_ratios['Total_TotalPapers'] = total_journal_papers_all_years\n",
        "            journal_ratios['Total_YesCount'] = total_top_institution_papers_all_years\n",
        "\n",
        "            yearly_ratios_data.append(journal_ratios) # Add journal's yearly ratios and counts to the list\n",
        "\n",
        "        yearly_ratios_df = pd.DataFrame(yearly_ratios_data) # Create DataFrame from yearly ratios data\n",
        "        yearly_ratios_df = yearly_ratios_df.set_index('Journal') # Set 'Journal' column as index for better readability\n",
        "\n",
        "        # Save the journal yearly ratios DataFrame to a new Excel file\n",
        "        ratios_filepath = yearly_ratios_df.to_excel(output_filepath, engine='openpyxl') # Index is now Journal, so index=True is default for index save\n",
        "        ratios_filepath = output_filepath # Correctly set ratios_filepath\n",
        "\n",
        "        print(f\"Journal yearly and total top institution ratios and counts saved to: {ratios_filepath}\")\n",
        "\n",
        "        # Optional: Download the journal yearly ratios spreadsheet\n",
        "        try:\n",
        "            files.download(output_filepath)\n",
        "            print(f\"Downloaded journal yearly ratios spreadsheet: {output_filepath}\")\n",
        "        except ImportError:\n",
        "            print(\"`files.download` not available (not in Colab). File is saved in Colab filesystem.\")\n",
        "\n",
        "        # Print ratios and counts to console in a table-like format\n",
        "        print(\"\\nJournal Yearly and Total Top Institution Ratios and Counts:\")\n",
        "        print(yearly_ratios_df.to_string(float_format=\"%.3f\")) # Use to_string for DataFrame print, format floats to 3 decimals\n",
        "\n",
        "        return ratios_filepath\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at: {input_filepath}. Make sure you ran Code Block 3 and the file was saved.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during yearly ratio and count calculation: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- EXECUTION CELL for STEP 6: Calculate and Output Journal YEARLY Ratios WITH COUNTS ---\n",
        "journal_yearly_ratios_filepath = calculate_journal_yearly_top_institution_ratios(input_filepath=\"labeled_papers.xlsx\", output_filepath=\"journal_yearly_ratios_with_counts.xlsx\")\n",
        "\n",
        "if journal_yearly_ratios_filepath:\n",
        "    print(f\"\\nJournal yearly and total ratio and count calculation completed. Yearly and total ratios and counts file saved at: {journal_yearly_ratios_filepath}\")\n",
        "else:\n",
        "    print(\"\\nJournal yearly and total ratio and count calculation process failed. See error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "7uNCp5XiHZK9",
        "outputId": "d286a8f8-b008-4e76-b71a-e9e304fea18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading labeled spreadsheet for yearly ratio and count calculation from: labeled_papers.xlsx\n",
            "Journal yearly and total top institution ratios and counts saved to: journal_yearly_ratios_with_counts.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6dbf0605-3e7d-424c-9caf-049ebed36bf5\", \"journal_yearly_ratios_with_counts.xlsx\", 5183)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded journal yearly ratios spreadsheet: journal_yearly_ratios_with_counts.xlsx\n",
            "\n",
            "Journal Yearly and Total Top Institution Ratios and Counts:\n",
            "                                     2023_Ratio  2023_TotalPapers  2023_YesCount  Total_Ratio  Total_TotalPapers  Total_YesCount\n",
            "Journal                                                                                                                         \n",
            "ACS Applied Materials & Interfaces        0.015              5071             76        0.015               5071              76\n",
            "Nanotechnology                            0.010               811              8        0.010                811               8\n",
            "Beilstein Journal of Nanotechnology       0.010               100              1        0.010                100               1\n",
            "\n",
            "Journal yearly and total ratio and count calculation completed. Yearly and total ratios and counts file saved at: journal_yearly_ratios_with_counts.xlsx\n"
          ]
        }
      ]
    }
  ]
}